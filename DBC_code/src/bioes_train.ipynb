{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "2.3.1\n",
      "0.7.7\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'to_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e0d161b16a81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbert4keras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbert4keras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnippets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msequence_padding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbert4keras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnippets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mViterbiDecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbert4keras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConditionalRandomField\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'to_array'"
     ]
    }
   ],
   "source": [
    "# 使用bioes标注体系标注数据，然后训练模型\n",
    "import tensorflow as tf\n",
    "import bert4keras\n",
    "import keras\n",
    "import os\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "print(bert4keras.__version__)\n",
    "import numpy as np\n",
    "from bert4keras.backend import keras, K\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "from bert4keras.optimizers import Adam\n",
    "from bert4keras.snippets import sequence_padding, DataGenerator\n",
    "from bert4keras.snippets import ViterbiDecoder, to_array\n",
    "from bert4keras.layers import ConditionalRandomField\n",
    "from keras.layers import Dense,Flatten\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm\n",
    "from utils import utils\n",
    "from imp import reload\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = utils.load_data('../data/train_bioes.txt')\n",
    "valid_data = utils.load_data('../data/val_bioes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_train_lst = []\n",
    "for data in train_data:\n",
    "    ret = 0\n",
    "    for char,_ in data:\n",
    "        ret += len(char)\n",
    "    ret_train_lst.append(ret)\n",
    "ret_val_lst = []\n",
    "for data in valid_data:\n",
    "    ret = 0\n",
    "    for char,_ in data:\n",
    "        ret += len(char)\n",
    "    ret_val_lst.append(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 6\n",
      "299 6\n",
      "2746\n",
      "278\n"
     ]
    }
   ],
   "source": [
    "print(max(ret_train_lst),min(ret_train_lst))\n",
    "print(max(ret_val_lst),min(ret_val_lst))\n",
    "print(len(ret_train_lst))\n",
    "print(len(ret_val_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练模型的超参数\n",
    "maxlen = 300\n",
    "epochs = 15\n",
    "batch_size = 24\n",
    "bert_layers = 12\n",
    "learing_rate = 1e-5 \n",
    "crf_lr_multiplier = 1000 \n",
    "rnn_lr_multiplier = 1000\n",
    "# bert配置,这里需要自己指定预训练模型的路径\n",
    "config_path = '../model/chinese_L-12_H-768_A-12/bert_config.json'\n",
    "checkpoint_path = '../model/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
    "dict_path = '../model/chinese_L-12_H-768_A-12/vocab.txt'\n",
    "# 建立分词器\n",
    "tokenizer = Tokenizer(dict_path, do_lower_case=True)\n",
    "\n",
    "# 类别映射\n",
    "labels = [\n",
    "    'position',\n",
    "    'name',\n",
    "    'organization',\n",
    "    'company',\n",
    "    'address',\n",
    "    'movie',\n",
    "    'game',\n",
    "    'government',\n",
    "    'scene',\n",
    "    'book',\n",
    "    'mobile',\n",
    "    'email',\n",
    "    'QQ',\n",
    "    'vx',\n",
    "]\n",
    "\n",
    "# 0 表示 'O'\n",
    "# 其他数字表示对应的 B 和 I\n",
    "id2label = dict(enumerate(labels))\n",
    "label2id = {j: i for i, j in id2label.items()}\n",
    "num_labels = len(labels) * 4 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'position': 0,\n",
       " 'name': 1,\n",
       " 'organization': 2,\n",
       " 'company': 3,\n",
       " 'address': 4,\n",
       " 'movie': 5,\n",
       " 'game': 6,\n",
       " 'government': 7,\n",
       " 'scene': 8,\n",
       " 'book': 9,\n",
       " 'mobile': 10,\n",
       " 'email': 11,\n",
       " 'QQ': 12,\n",
       " 'vx': 13}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id2label\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_transformer_model(\n",
    "    config_path,\n",
    "    checkpoint_path,\n",
    ")\n",
    "\n",
    "output_layer = 'Transformer-%s-FeedForward-Norm' % (bert_layers - 1)\n",
    "output = model.get_layer(output_layer).output\n",
    "output_for_crf = Dense(num_labels)(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     (None, None, 768)    16226304    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          \n",
      "                                                                 Embedding-Dropout[0][0]          \n",
      "                                                                 Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          \n",
      "                                                                 Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
      "                                                                 Transformer-0-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-0-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
      "                                                                 Transformer-1-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-1-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
      "                                                                 Transformer-2-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-FeedForward-Norm[0]\n",
      "                                                                 Transformer-2-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]\n",
      "                                                                 Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
      "                                                                 Transformer-3-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]\n",
      "                                                                 Transformer-3-FeedForward-Norm[0]\n",
      "                                                                 Transformer-3-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]\n",
      "                                                                 Transformer-4-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
      "                                                                 Transformer-4-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]\n",
      "                                                                 Transformer-4-FeedForward-Norm[0]\n",
      "                                                                 Transformer-4-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]\n",
      "                                                                 Transformer-5-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
      "                                                                 Transformer-5-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]\n",
      "                                                                 Transformer-5-FeedForward-Norm[0]\n",
      "                                                                 Transformer-5-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]\n",
      "                                                                 Transformer-6-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
      "                                                                 Transformer-6-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]\n",
      "                                                                 Transformer-6-FeedForward-Norm[0]\n",
      "                                                                 Transformer-6-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]\n",
      "                                                                 Transformer-7-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
      "                                                                 Transformer-7-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]\n",
      "                                                                 Transformer-7-FeedForward-Norm[0]\n",
      "                                                                 Transformer-7-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]\n",
      "                                                                 Transformer-8-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
      "                                                                 Transformer-8-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]\n",
      "                                                                 Transformer-8-FeedForward-Norm[0]\n",
      "                                                                 Transformer-8-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]\n",
      "                                                                 Transformer-9-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
      "                                                                 Transformer-9-FeedForward-Dropout\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]\n",
      "                                                                 Transformer-9-FeedForward-Norm[0]\n",
      "                                                                 Transformer-9-FeedForward-Norm[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]\n",
      "                                                                 Transformer-10-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
      "                                                                 Transformer-10-FeedForward-Dropou\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0\n",
      "                                                                 Transformer-10-FeedForward-Norm[0\n",
      "                                                                 Transformer-10-FeedForward-Norm[0\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0\n",
      "                                                                 Transformer-11-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
      "                                                                 Transformer-11-FeedForward-Dropou\n",
      "__________________________________________________________________________________________________\n",
      "Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_659 (Dense)               (None, None, 57)     43833       Transformer-11-FeedForward-Norm[0\n",
      "__________________________________________________________________________________________________\n",
      "conditional_random_field_5 (Con (None, None, 57)     3249        dense_659[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 101,724,138\n",
      "Trainable params: 101,724,138\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 使用crf进行训练\n",
    "CRF = ConditionalRandomField(lr_multiplier=crf_lr_multiplier)\n",
    "output_crf = CRF(output_for_crf)\n",
    "\n",
    "model = Model(model.input, [output_crf])\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss=[CRF.sparse_loss,],\n",
    "    optimizer=Adam(learing_rate),\n",
    "    metrics=[CRF.sparse_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Layer flatten_7 does not support masking, but was passed an input_mask: Tensor(\"Transformer-11-FeedForward-Add_28/All:0\", shape=(None, None), dtype=bool)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-bfde3ef99de4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# 取出[CLS]对应的向量用来做分类\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# note:这里\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m# (None, 768)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# print('output:',output.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WinAPP\\dev_app\\anaconda\\envs\\ner\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WinAPP\\dev_app\\anaconda\\envs\\ner\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[1;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[1;31m# If the layer returns tensors from its inputs, unmodified,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WinAPP\\dev_app\\anaconda\\envs\\ner\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mcompute_mask\u001b[1;34m(self, inputs, mask)\u001b[0m\n\u001b[0;32m    646\u001b[0m                                     \u001b[1;34m' does not support masking, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m                                     \u001b[1;34m'but was passed an input_mask: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m                                     str(mask))\n\u001b[0m\u001b[0;32m    649\u001b[0m             \u001b[1;31m# masking not explicitly supported: return None as mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Layer flatten_7 does not support masking, but was passed an input_mask: Tensor(\"Transformer-11-FeedForward-Add_28/All:0\", shape=(None, None), dtype=bool)"
     ]
    }
   ],
   "source": [
    "# # 使用softmax损失函数进行训练\n",
    "# model = build_transformer_model(\n",
    "#     config_path,\n",
    "#     checkpoint_path,\n",
    "# )\n",
    "\n",
    "# output_layer = 'Transformer-%s-FeedForward-Norm' % (bert_layers - 1)\n",
    "# output = model.get_layer(output_layer).output\n",
    "# # print('output:',output.shape) # (None, None, 768)  [batch_size,seq_len,hidden_size],这里768=12*64(12个head，每个head有64个隐向量)\n",
    "# # [batch_size,seq_len,hidden_size]\n",
    "# # output = Dense(num_labels)(output)\n",
    "# # 取出[CLS]对应的向量用来做分类\n",
    "# # note:这里\n",
    "# output = Flatten()(output)\n",
    "# # (None, 768)\n",
    "# # print('output:',output.shape)\n",
    "# output_for_softmax = Dense(units=num_labels,activation='softmax')(output)\n",
    "# # print('output_for_softmax:',output_for_softmax.shape)  #(None, 57)\n",
    "\n",
    "# model = Model(model.input, output_for_softmax)\n",
    "# model.summary()\n",
    "\n",
    "# model.compile(\n",
    "#     loss='categorical_crossentropy',\n",
    "#     optimizer=Adam(learing_rate),\n",
    "#     metrics=['accuracy']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator(DataGenerator):\n",
    "    \"\"\"\n",
    "    数据生成器\n",
    "    \"\"\"\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "        # is_end  是否到 epoch 的末尾， item 对应的 train_data[i]\n",
    "        for is_end, item in self.sample(random):\n",
    "#             print('item:',item) ['布莱克本', 'organization'], ['对', 'O']\n",
    "            # vocab.txt 从 0 开始计数，0:[pad] 1-99:[unused*] 100:[UNK] [CLS]:101\n",
    "            # token_ids:101 ，label：[0]在词表中0代表的是pad\n",
    "            token_ids, labels = [tokenizer._token_start_id], [0]\n",
    "            for token, token_label in item:\n",
    "#                 print('token, token_label:',token, token_label)\n",
    "                # 得到除了 [CLS] [SEP] 之外的每一个词的 ids\n",
    "                # e.g '全国青联委员经纪人' -> [1059, 1744, 7471, 5468, 1999, 1447, 5307, 5279, 782]\n",
    "                # [CLS]word1 word2,...,word_n[SEP]                 \n",
    "                w_token_ids = tokenizer.encode(token)[0][1:-1]\n",
    "                if len(token_ids) + len(w_token_ids) < maxlen:\n",
    "                    # e.g [101, 1059, 1744, 7471, 5468, 1999, 1447, 5307, 5279, 782]\n",
    "                    # 把 [CLS] 加在前面\n",
    "                    token_ids += w_token_ids\n",
    "                    # BIOES\n",
    "                    if token_label == 'O':\n",
    "                        # 如果是 'O' 则标签是 0\n",
    "                        labels += [0] * len(w_token_ids)\n",
    "                    else:\n",
    "                        # 如果不是 'O' 生成 label2id 对应的标签\n",
    "                        # e.g '全国青联委员经纪人' -> [0, 1, 2, 2, 2, 2, 2, 2, 2, 2] 注意：第一个 [CLS] 默认是 0\n",
    "                        token_len = len(w_token_ids)\n",
    "                        if token_len == 1:\n",
    "                            S = label2id[token_label] * 4 + 4 \n",
    "                            labels += ([S])\n",
    "                        elif token_len == 2:\n",
    "                            B = label2id[token_label] * 4 + 1\n",
    "                            E = label2id[token_label] * 4 + 3\n",
    "                            labels += ([B] + [E])\n",
    "                        else:\n",
    "                            B = label2id[token_label] * 4 + 1\n",
    "                            I = label2id[token_label] * 4 + 2\n",
    "                            E = label2id[token_label] * 4 + 3\n",
    "                            labels += ([B] + [I] * (len(w_token_ids) - 2)+[E])\n",
    "#                             labels += ([B] + [I] * (len(w_token_ids) - 1))\n",
    "#                     print('')\n",
    "#                         print('label:',token_label, 'label2id:',label2id[token_label])\n",
    "#                         print('labels:',labels)\n",
    "                else:\n",
    "                    # 超过 maxlen 长度的部分被舍弃掉\n",
    "                    break\n",
    "#                 print('+' * 100)\n",
    "            # 加上尾巴 [SEP]:102\n",
    "            token_ids += [tokenizer._token_end_id]\n",
    "            # 尾巴的 label 也是 0\n",
    "            labels += [0]\n",
    "            # 生成 segment_ids ，NER 只用了一个句子，所以这里都是 0，这里是数字0，而不是字母O\n",
    "            segment_ids = [0] * len(token_ids)\n",
    "            # 把样本组装成 batch\n",
    "            batch_token_ids.append(token_ids)\n",
    "            batch_segment_ids.append(segment_ids)\n",
    "            batch_labels.append(labels)\n",
    "            # 如果凑齐了 batchsize 个，或者到 epoch 的最后一个，那么就返回。\n",
    "            if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                # 进行 padding 操作\n",
    "                # [bs,seq] 默认按照 bs 中最大的长度进行填充，保证每个 bs 的长度是一致，用 0 填充\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                batch_labels = sequence_padding(batch_labels)\n",
    "                yield [batch_token_ids, batch_segment_ids], batch_labels\n",
    "                batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
    "\n",
    "\n",
    "\n",
    "class NamedEntityRecognizer(ViterbiDecoder):\n",
    "    \"\"\"\n",
    "    命名实体识别器\n",
    "    \"\"\"\n",
    "    def recognize(self, text):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        mapping = tokenizer.rematch(text, tokens)\n",
    "        token_ids = tokenizer.tokens_to_ids(tokens)\n",
    "        segment_ids = [0] * len(token_ids)\n",
    "        token_ids, segment_ids = to_array([token_ids], [segment_ids])\n",
    "        # [batch_size,seq_len,label]==>[1,80,57]\n",
    "        # 这里就输入了一个句子，所以batch_size = 1,\n",
    "        # model.predict([token_ids, segment_ids])[0] 这里取出的是后面两个维度 [seq_len,label],label的个数是确定的，就是上面的4* __ +1\n",
    "        nodes = model.predict([token_ids, segment_ids])[0]\n",
    "        # dp解码寻找最优路径\n",
    "        # 长度为 seq 的向量，labels 每个值表示标签值\n",
    "        labels = self.decode(nodes)\n",
    "        entities, starting = [], False\n",
    "        for i, label in enumerate(labels):\n",
    "            if label > 0: # BIES\n",
    "                if label % 4 == 1:\n",
    "                    # 对应的B，这里和上面的 BIOES 的下标对应\n",
    "                    # 有了 B 意味着我们开始预测实体了，所以starting = True，就需要把这个预测结果放到对应的\n",
    "                    # 集合中，\n",
    "                    starting = True\n",
    "                    # 把对应的下标位置 i,对应的标签存下来\n",
    "                    entities.append([[i], id2label[(label - 1) // 4]])\n",
    "                elif label%4 == 0: # 对应的S,预测为S,后面就不需要预测了，所以 starting = False\n",
    "                    starting = False\n",
    "                    entities.append([[i], id2label[(label - 4) // 4]])\n",
    "                elif starting:\n",
    "                    # 预测的结果不是 B,不是S，只能是I,E,把当前实体信息拿出来，把对应的角标存下来\n",
    "                    # entities的结构：[[[i], id2label[(label - 1) // 4]],[[[i], id2label[(label - 1) // 4]]]]\n",
    "                    # entities[-1][0]即 [i]这个部分\n",
    "                    entities[-1][0].append(i)\n",
    "                else:\n",
    "                    starting = False\n",
    "            else:\n",
    "                starting = False\n",
    "        # 把预测的结果还原成文本形式\n",
    "        # \n",
    "        return [(text[mapping[w[0]][0]:mapping[w[-1]][-1] + 1], l) for w, l in entities]\n",
    "\n",
    "# len(K.eval(CRF.trans)):标签类别数\n",
    "# 除了 0 ，其余标签都是 non_starts,non_ends\n",
    "NER = NamedEntityRecognizer(trans=K.eval(CRF.trans), starts=[0], ends=[0])\n",
    "\n",
    "tp = defaultdict(int)\n",
    "predict_true = defaultdict(int)\n",
    "gt = defaultdict(int)\n",
    "# 各个类别p，r值的存放\n",
    "pr_dict = defaultdict(dict)\n",
    "wrong_dict = defaultdict(list)\n",
    "\n",
    "def calc_F1_for_every_cls(R, T):\n",
    "    \"\"\"R 预测标签，T真实标签 ，分别统计标签不匹配，entity不匹配，以及二者都不匹配的情况\n",
    "    \"\"\"\n",
    "    for r_entity, r_label in R:\n",
    "        predict_true[r_label] += 1\n",
    "    for t_entity, t_label in T:\n",
    "        gt[t_label] += 1\n",
    "    for r_entity, r_label in R:\n",
    "        for t_entity, t_label in T:\n",
    "            if r_entity == t_entity and r_label == t_label:\n",
    "                tp[r_label] += 1\n",
    "            # 把预测错误的数据保存下来，\n",
    "            elif r_entity != t_entity and r_label == t_label:\n",
    "                for label in labels:\n",
    "                    if label == r_label:\n",
    "                        wrong_dict['entity_wrong_'+label].append([(r_entity, r_label),(r_entity, r_label)])\n",
    "            elif r_entity == t_entity and r_label != t_label:\n",
    "                for label in labels:\n",
    "                    if label==r_label:\n",
    "                        wrong_dict['label_wrong_'+label].append([(r_entity, r_label),(r_entity, r_label)])\n",
    "            else:\n",
    "                # 标签，entity都预测错误的就没办法分类了\n",
    "                wrong_dict['all_wrong'].append(([(r_entity, r_label),(r_entity, r_label)]))\n",
    "                \n",
    "    with open('wrong_predict.txt','a',encoding='utf-8') as f:\n",
    "        for label in labels:\n",
    "            for item in wrong_dict['entity_wrong_'+label]:\n",
    "                f.write(str(item)+'\\n')\n",
    "        f.write('\\n')\n",
    "        \n",
    "        for label in labels:\n",
    "            for item in wrong_dict['label_wrong_'+label]:\n",
    "                f.write(str(item)+'\\n')\n",
    "        f.write('\\n')\n",
    "        \n",
    "        for item in wrong_dict['all_wrong']:\n",
    "            f.write(str(item)+'\\n')\n",
    "        f.write('\\n\\n')\n",
    "                \n",
    "def evaluate(data):\n",
    "    \"\"\"评测函数\n",
    "    \"\"\"\n",
    "    X, Y, Z = 1e-10, 1e-10, 1e-10\n",
    "    \n",
    "    for d in tqdm(data):\n",
    "        text = ''.join([i[0] for i in d])\n",
    "        R = set(NER.recognize(text)) # 预测\n",
    "        T = set([tuple(i) for i in d if i[1] != 'O']) #真实\n",
    "        # 计算每个类别的p,r值，并把预测错误的结果保存下来，同时统计每个类别的数据量，数据不平衡问题如何处理\n",
    "        # 一个样本里面预测的结果\n",
    "        # R: {('美国', 'address'), ('War3', 'game'), ('SC2', 'game'), ('暴雪', 'company'), ('DotA', 'game')}\n",
    "        # T: {('美国', 'address'), ('War3', 'game'), ('SC2', 'game'), ('暴雪', 'company'), ('DotA', 'game')}\n",
    "        calc_F1_for_every_cls(R,T)\n",
    "        X += len(R & T) \n",
    "        Y += len(R) \n",
    "        Z += len(T)\n",
    "    \n",
    "    for label in labels:\n",
    "        if predict_true[label]!=0:\n",
    "          pr_dict['precision'][label] = round(tp[label] / predict_true[label],5)\n",
    "        if gt[label]!=0:\n",
    "          pr_dict['recall'][label] = round(tp[label] / gt[label],5)\n",
    "    precision, recall =  X / Y, X / Z\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    return f1, precision, recall,pr_dict\n",
    "\n",
    "\n",
    "class Evaluator(keras.callbacks.Callback):\n",
    "    def __init__(self,valid_data):\n",
    "        self.best_val_f1 = 0\n",
    "        self.valid_data = valid_data\n",
    "        print('valid_data,',len(valid_data))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "#         print('epoch:',epoch)\n",
    "        trans = K.eval(CRF.trans)\n",
    "        NER.trans = trans\n",
    "#         print(NER.trans)\n",
    "        f1, precision, recall,pr_dict = evaluate(self.valid_data)\n",
    "        # 保存最优\n",
    "        if f1 >= self.best_val_f1:\n",
    "            self.best_val_f1 = f1\n",
    "            model.save_weights('../model/best_model_bioes.weights')\n",
    "        print(\n",
    "            'valid:  f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f' %\n",
    "            (f1, precision, recall, self.best_val_f1)\n",
    "        )\n",
    "        print('precision for every class:',pr_dict['precision'])\n",
    "        print('recall for every class:',pr_dict['recall'])\n",
    "\n",
    "\n",
    "\n",
    "evaluator = Evaluator(valid_data)\n",
    "train_generator = data_generator(train_data, batch_size)\n",
    "model.fit_generator(\n",
    "    train_generator.forfit(),\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=epochs,\n",
    "    callbacks=[evaluator]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner",
   "language": "python",
   "name": "ner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
