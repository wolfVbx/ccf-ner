{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from transformers_models.modeling_bert import BertPreTrainedModel,BertModel\n",
    "from transformers_models import WEIGHTS_NAME, BertConfig,BertTokenizer\n",
    "\n",
    "from callbacks.adamw import AdamW\n",
    "from callbacks.lr_scheduler import get_linear_schedule_with_warmup\n",
    "from callbacks.progressbar import ProgressBar\n",
    "\n",
    "from utils.utils_ner import CNerTokenizer, get_entities,SeqEntityScore\n",
    "from utils.ner_seq import convert_examples_to_features,collate_fn\n",
    "from utils.ner_seq import ner_processors as processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1029):\n",
    "    '''\n",
    "    设置整个开发环境的seed\n",
    "    :param seed:\n",
    "    :param device:\n",
    "    :return:\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # some cudnn methods can be random even after fixing the seed\n",
    "    # unless you tell it to be deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class Config():\n",
    "    model_type='bert' \n",
    "    task_name='cluener' \n",
    "    data_dir='datasets' \n",
    "    model_name_or_path='./pre_trained_model/bert-base'\n",
    "    markup = 'bios'\n",
    "    train_max_seq_length = 300\n",
    "    eval_max_seq_length = 300\n",
    "    do_train = True\n",
    "    do_eval = True\n",
    "    do_predict = True\n",
    "    do_lower_case = True\n",
    "    per_gpu_train_batch_size = 8\n",
    "    per_gpu_eval_batch_size = 8\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 3e-5\n",
    "\n",
    "    weight_decay = 0.01\n",
    "    adam_epsilon = 1e-8\n",
    "    max_grad_norm = 1.0\n",
    "    num_train_epochs = 3\n",
    "    max_steps= -1\n",
    "    warmup_proportion = 0.1\n",
    "    overwrite_output_dir=True\n",
    "    output_dir='outputs/' \n",
    "    seed=42\n",
    "    save_steps=448 \n",
    "    logging_steps=448 \n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSoftmaxForNer(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertSoftmaxForNer, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.loss_type = config.loss_type\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n",
    "                position_ids=None, head_mask=None, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=0)\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "\n",
    "                active_loss = attention_mask.contiguous().view(-1) == 1\n",
    "\n",
    "                active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
    "                active_labels = labels.contiguous().view(-1)[active_loss]\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "        return outputs  # (loss), scores, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNerTokenizer(BertTokenizer):\n",
    "    def __init__(self, vocab_file, do_lower_case=False):\n",
    "        super().__init__(vocab_file=str(vocab_file), do_lower_case=do_lower_case)\n",
    "        self.vocab_file = str(vocab_file)\n",
    "        self.do_lower_case = do_lower_case\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        _tokens = []\n",
    "        for c in text:\n",
    "            if self.do_lower_case:\n",
    "                c = c.lower()\n",
    "            if c in self.vocab:\n",
    "                _tokens.append(c)\n",
    "            else:\n",
    "                _tokens.append('[UNK]')\n",
    "        return _tokens\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertSoftmaxForNer, CNerTokenizer),\n",
    "}\n",
    "\n",
    "path = './pre_trained_model/bert-base'\n",
    "# path = r'C:\\Users\\wvbx\\.cache\\torch\\transformers\\bert-base-chinese'\n",
    "tokenizer = CNerTokenizer.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model, tokenizer):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size,\n",
    "                              collate_fn=collate_fn)\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_loader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_loader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         \"weight_decay\": args.weight_decay, },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    args.warmup_steps = int(t_total * args.warmup_proportion)\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps,\n",
    "                                                num_training_steps=t_total)\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
    "            os.path.join(args.model_name_or_path, \"scheduler.pt\")):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "    \n",
    "    # Train!\n",
    "    print(\"***** Running training *****\")\n",
    "    print(\"  Num examples = %d\", len(train_dataset))\n",
    "    print(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    print(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    print(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "                args.train_batch_size* args.gradient_accumulation_steps,\n",
    "                )\n",
    "    print(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    print(\"  Total optimization steps = %d\", t_total)\n",
    "    global_step = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if os.path.exists(args.model_name_or_path) and \"checkpoint\" in args.model_name_or_path:\n",
    "        # set global_step to gobal_step of last saved checkpoint from model path\n",
    "        global_step = int(args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0])\n",
    "        epochs_trained = global_step // (len(train_loader) // args.gradient_accumulation_steps)\n",
    "        steps_trained_in_current_epoch = global_step % (len(train_loader) // args.gradient_accumulation_steps)\n",
    "        print(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "        print(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "        print(\"  Continuing training from global step %d\", global_step)\n",
    "        print(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    seed_everything(args.seed)  # Added here for reproductibility (even between python 2 and 3)\n",
    "    for _ in range(int(args.num_train_epochs)):\n",
    "        pbar = ProgressBar(n_total=len(train_loader), desc='Training')\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "            model.train()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            if args.model_type != \"distilbert\":\n",
    "                # XLM and RoBERTa don\"t use segment_ids\n",
    "                inputs[\"token_type_ids\"] = (batch[2] if args.model_type in [\"bert\", \"xlnet\"] else None)\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "            else:\n",
    "                loss.backward()\n",
    "            pbar(step, {'loss': loss.item()})\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    print(\" \")\n",
    "                    evaluate(args, model, tokenizer)\n",
    "\n",
    "                if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    # Take care of distributed/parallel training\n",
    "                    model_to_save = (model.module if hasattr(model, \"module\") else model)\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                    tokenizer.save_vocabulary(output_dir)\n",
    "                    print(\"Saving model checkpoint to %s\", output_dir)\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                    print(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "        printo(\"\\n\")\n",
    "        if 'cuda' in str(args.device):\n",
    "            torch.cuda.empty_cache()\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
    "    metric = SeqEntityScore(args.id2label, markup=args.markup)\n",
    "    eval_output_dir = args.output_dir\n",
    "    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
    "        os.makedirs(eval_output_dir)\n",
    "    eval_dataset = load_and_cache_examples(args, args.task_name, tokenizer, data_type='dev')\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,\n",
    "                                 collate_fn=collate_fn)\n",
    "    # Eval!\n",
    "    print(\"***** Running evaluation %s *****\", prefix)\n",
    "    print(\"  Num examples = %d\", len(eval_dataset))\n",
    "    print(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    pbar = ProgressBar(n_total=len(eval_dataloader), desc=\"Evaluating\")\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            if args.model_type != \"distilbert\":\n",
    "                # XLM and RoBERTa don\"t use segment_ids\n",
    "                inputs[\"token_type_ids\"] = (batch[2] if args.model_type in [\"bert\", \"xlnet\"] else None)\n",
    "            outputs = model(**inputs)\n",
    "        tmp_eval_loss, logits = outputs[:2]\n",
    "        if args.n_gpu > 1:\n",
    "            tmp_eval_loss = tmp_eval_loss.mean()  # mean() to average on multi-gpu parallel evaluating\n",
    "        eval_loss += tmp_eval_loss.item()\n",
    "        nb_eval_steps += 1\n",
    "        preds = np.argmax(logits.cpu().numpy(), axis=2).tolist()\n",
    "        out_label_ids = inputs['labels'].cpu().numpy().tolist()\n",
    "        input_lens = batch[4].cpu().numpy().tolist()\n",
    "        for i, label in enumerate(out_label_ids):\n",
    "            temp_1 = []\n",
    "            temp_2 = []\n",
    "            for j, m in enumerate(label):\n",
    "                if j == 0:\n",
    "                    continue\n",
    "                elif j == input_lens[i] - 1:\n",
    "                    metric.update(pred_paths=[temp_2], label_paths=[temp_1])\n",
    "                    break\n",
    "                else:\n",
    "                    temp_1.append(args.id2label[out_label_ids[i][j]])\n",
    "                    temp_2.append(preds[i][j])\n",
    "        pbar(step)\n",
    "    print(\"\\n\")\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_info, entity_info = metric.result()\n",
    "    results = {f'{key}': value for key, value in eval_info.items()}\n",
    "    results['loss'] = eval_loss\n",
    "    print(\"***** Eval results %s *****\", prefix)\n",
    "    info = \"-\".join([f' {key}: {value:.4f} ' for key, value in results.items()])\n",
    "    print(info)\n",
    "    print(\"***** Entity results %s *****\", prefix)\n",
    "    for key in sorted(entity_info.keys()):\n",
    "        print(\"******* %s results ********\" % key)\n",
    "        info = \"-\".join([f' {key}: {value:.4f} ' for key, value in entity_info[key].items()])\n",
    "        print(info)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(args, model, tokenizer, prefix=\"\"):\n",
    "    pred_output_dir = './output'\n",
    "    if not os.path.exists(pred_output_dir) and args.local_rank in [-1, 0]:\n",
    "        os.makedirs(pred_output_dir)\n",
    "\n",
    "    test_dataset = load_and_cache_examples(args, args.task_name, tokenizer, data_type='test')\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    test_sampler = SequentialSampler(test_dataset) if args.local_rank == -1 else DistributedSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=1, collate_fn=collate_fn)\n",
    "    # Eval!\n",
    "    print(\"***** Running prediction %s *****\", prefix)\n",
    "    print(\"  Num examples = %d\", len(test_dataset))\n",
    "    print(\"  Batch size = %d\", 1)\n",
    "\n",
    "    results = []\n",
    "    output_submit_file = os.path.join(pred_output_dir, prefix, \"test_prediction.json\")\n",
    "    pbar = ProgressBar(n_total=len(test_dataloader), desc=\"Predicting\")\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": None}\n",
    "            inputs[\"token_type_ids\"] = batch[2]\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "        preds = logits.detach().cpu().numpy()\n",
    "        preds = np.argmax(preds, axis=2).tolist()\n",
    "        preds = preds[0][1:-1]  # [CLS]XXXX[SEP]\n",
    "        tags = [args.id2label[x] for x in preds]\n",
    "        label_entities = get_entities(preds, args.id2label, args.markup)\n",
    "        json_d = {}\n",
    "        json_d['id'] = step\n",
    "        json_d['tag_seq'] = \" \".join(tags)\n",
    "        json_d['entities'] = label_entities\n",
    "        # 把概率值也输出来\n",
    "        json_d['prob_value'] = preds\n",
    "        results.append(json_d)\n",
    "        pbar(step)\n",
    "    print(\"\\n\")\n",
    "    with open(output_submit_file, \"w\") as writer:\n",
    "        for record in results:\n",
    "            writer.write(json.dumps(record) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, task, tokenizer, data_type='train'):\n",
    "    processor = processors[task]()\n",
    "    # Load data features from cache or dataset file\n",
    "    cached_features_file = os.path.join(args.data_dir, 'cached_soft-{}_{}_{}_{}.txt'.format(\n",
    "        data_type,\n",
    "        list(filter(None, args.model_name_or_path.split('/'))).pop(),\n",
    "        str(args.train_max_seq_length if data_type == 'train' else args.eval_max_seq_length),\n",
    "        str(task)))\n",
    "    if os.path.exists(cached_features_file):\n",
    "        print(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        print(\"Creating features from dataset file at %s\", args.data_dir)\n",
    "        label_list = processor.get_labels()\n",
    "        if data_type == 'train':\n",
    "            examples = processor.get_train_examples(args.data_dir)\n",
    "        elif data_type == 'dev':\n",
    "            examples = processor.get_dev_examples(args.data_dir)\n",
    "        else:\n",
    "            examples = processor.get_test_examples(args.data_dir)\n",
    "        features = convert_examples_to_features(examples=examples,\n",
    "                            tokenizer=tokenizer,\n",
    "                            label_list=label_list,\n",
    "                            max_seq_length=args.train_max_seq_length if data_type == 'train' \\\n",
    "                                else args.eval_max_seq_length,\n",
    "                            cls_token_at_end=False,\n",
    "                            pad_on_left=False,\n",
    "                            cls_token=tokenizer.cls_token,\n",
    "                            cls_token_segment_id = 0,\n",
    "                            sep_token=tokenizer.sep_token,\n",
    "                            # pad on the left for xlnet\n",
    "                            pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "                            pad_token_segment_id= 0,)\n",
    "\n",
    "        print(\"Saving features into cached file %s\", cached_features_file)\n",
    "        torch.save(features, cached_features_file)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "    all_lens = torch.tensor([f.input_len for f in features], dtype=torch.long)\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_lens, all_label_ids)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.mkdir(args.output_dir)\n",
    "    args.output_dir = args.output_dir + '{}'.format(args.model_type)\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.mkdir(args.output_dir)\n",
    "    if os.path.exists(args.output_dir) and os.listdir(\n",
    "            args.output_dir) and args.do_train and not args.overwrite_output_dir:\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                args.output_dir))\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    args.n_gpu = 1\n",
    "    args.device = device\n",
    "    # Set seed\n",
    "    seed_everything(args.seed)\n",
    "    # Prepare NER task\n",
    "    args.task_name = args.task_name.lower()\n",
    "    if args.task_name not in processors:\n",
    "        raise ValueError(\"Task not found: %s\" % (args.task_name))\n",
    "    processor = processors[args.task_name]()\n",
    "    label_list = processor.get_labels()\n",
    "    args.id2label = {i: label for i, label in enumerate(label_list)}\n",
    "    args.label2id = {label: i for i, label in enumerate(label_list)}\n",
    "    num_labels = len(label_list)\n",
    "    # Load pretrained model and tokenizer\n",
    "    args.model_type = args.model_type.lower()\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES['bert']\n",
    "    config = config_class.from_pretrained(args.model_name_or_path,num_labels=num_labels,loss_type='ce')\n",
    "    tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)\n",
    "    \n",
    "    model = model_class.from_pretrained(args.model_name_or_path, from_tf=False,config=config)\n",
    "    model.to(args.device)\n",
    "    print(\"Training/evaluation parameters %s\", args)\n",
    "    # Training\n",
    "\n",
    "    train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, data_type='train')\n",
    "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "    print(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "    if args.do_train :\n",
    "        # Create output directory if needed\n",
    "        if not os.path.exists(args.output_dir):\n",
    "            os.makedirs(args.output_dir)\n",
    "        print(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_vocabulary(args.output_dir)\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if args.do_eval:\n",
    "        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n",
    "        checkpoints = [args.output_dir]\n",
    "        if args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "        print(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else \"\"\n",
    "            model = model_class.from_pretrained(checkpoint)\n",
    "            model.to(args.device)\n",
    "            result = evaluate(args, model, tokenizer, prefix=prefix)\n",
    "            if global_step:\n",
    "                result = {\"{}_{}\".format(global_step, k): v for k, v in result.items()}\n",
    "            results.update(result)\n",
    "        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            for key in sorted(results.keys()):\n",
    "                writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n",
    "\n",
    "    if args.do_predict:\n",
    "        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n",
    "        checkpoints = [args.output_dir]\n",
    "        if args.predict_checkpoints > 0:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
    "            checkpoints = [x for x in checkpoints if x.split('-')[-1] == str(args.predict_checkpoints)]\n",
    "        print(\"Predict the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else \"\"\n",
    "            model = model_class.from_pretrained(checkpoint)\n",
    "            model.to(args.device)\n",
    "            predict(args, model, tokenizer, prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/evaluation parameters %s <__main__.Config object at 0x00000233FFF97EB8>\n",
      "Loading features from cached file %s datasets\\cached_soft-train_bert-base_300_cluener.txt\n",
      "***** Running training *****\n",
      "  Num examples = %d 10748\n",
      "  Num Epochs = %d 3\n",
      "  Instantaneous batch size per GPU = %d 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = %d 8\n",
      "  Gradient Accumulation steps = %d 1\n",
      "  Total optimization steps = %d 4032\n",
      "[Training] 1/1344 [..............................] - ETA: 38:37  loss: 3.7192 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\WinAPP\\dev_app\\anaconda\\envs\\ner\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] 19/1344 [..............................] - ETA: 1:01:57  loss: 3.6731 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-110-c08f32e005a3>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_and_cache_examples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" global_step = %s, average loss = %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-106-3217cc0584e2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args, train_dataset, model, tokenizer)\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m                 \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Update learning rate schedule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WinAPP\\dev_app\\anaconda\\envs\\ner\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mparam_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WinAPP\\dev_app\\anaconda\\envs\\ner\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"fro\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;34mr\"\"\"See :func:`torch.norm`\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpivot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_infos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WinAPP\\dev_app\\anaconda\\envs\\ner\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m    745\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"nuc\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"fro\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner",
   "language": "python",
   "name": "ner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
